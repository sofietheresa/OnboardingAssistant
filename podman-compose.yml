version: "3.8"

services:
  litellm:
    image: ghcr.io/berriai/litellm-database:main-v1.37.13-stable
    container_name: chat-litellm
    ports:
      - "4000:4000"
    env_file:
      - ./.env
    volumes:
      - ./litellm-config.yaml:/app/config.yaml:Z
    command: [
      "--config", "/app/config.yaml",
      "--port", "4000",
      "--debug",
      "--detailed_debug",
      "--add_function_to_prompt",
      "--num_workers", "4",
      "--telemetry", "False"
    ]
    restart: unless-stopped

  backend:
    build: ./backend
    container_name: chat-backend
    ports:
      - "8000:8000"
    env_file:
      - ./.env
    depends_on:
      - litellm
    command: ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
    restart: unless-stopped 